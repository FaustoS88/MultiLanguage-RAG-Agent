# Context7 Crawler & Vector Loader

An overview of how the context7 crawler works, in four clear stages.

---

## 1. Crawling

- **Targets:** A list of Context7 library paths (`CONTEXT7_LIBRARY_IDS`).
- **Fetcher:** `crawl4ai.AsyncWebCrawler` sends HTTP requests to each `/llms.txt` URL.
- **Output:** Raw text content of each `llms.txt` file.

---

## 2. Parsing

- **Separator:** Splits on a line of dashes (`----------------------------------------`).
- **Extraction:** For each segment, uses regex to find:
  - `TITLE:`
  - `DESCRIPTION:`
  - `SOURCE:` (URL or autogenerated placeholder)
  - `LANGUAGE:`
  - `CODE:` block
- **Result:** A list of snippet objects, each with metadata and code.

---

## 3. Embedding

- **Model:** Calls OpenAI’s embeddings API (`text-embedding-3-small`).
- **Input:** Combined snippet text: Title, Description, Language, Code.
- **Limits:** Skips or zeroes out vectors for empty or overly long snippets.
- **Output:** A 1,536-dimensional numeric vector per snippet.

---

## 4. Storage

- **Database:** Postgres with the `pgvector` extension.
- **Insert:** Uses `psycopg2.extras.execute_values` for bulk writes.
- **Idempotency:** `ON CONFLICT (library_id, source_url) DO NOTHING` prevents duplicates.

---

## Configuration

Set environment variables in a `.env` file:

```
DB_NAME=...
DB_USER=...
DB_PASSWORD=...
DB_HOST=...
DB_PORT=...
OPENAI_API_KEY=...
EMBEDDING_MODEL=text-embedding-3-small
```

Ensure your Postgres table exists with:

```sql
CREATE EXTENSION IF NOT EXISTS pgvector;
CREATE TABLE context7_docs (
  library_id TEXT,
  source_url TEXT,
  title TEXT,
  description TEXT,
  language TEXT,
  code TEXT,
  embedding VECTOR(1536),
  PRIMARY KEY (library_id, source_url)
);
```

---

## Tips & Extensions

- **Separator Flexibility:** Adjust the dash count or use a regex to match your separator.
- **Error Handling:** Add retry/back-off for rate limits on embeddings.
- **Updates:** Change the `ON CONFLICT` clause if you need to refresh embeddings on each run.

---

# Context7 Crawler: Quick Reference

This document describes how to manage your list of Context7 libraries and how the Postgres upsert behavior works.

---

## 1. Library List Management

- **Primary source:**  
  The crawler reads the Python constant

  ```python
  CONTEXT7_LIBRARY_IDS = [
    "/angular/angular",
    "/numpy/numpy.org",
    ...
  ]
  ```

  at the top of `context7_extractor.py`.

- **Adding new libraries:**  
  Simply replace or extend this list. On next run, any new `(library_id, source_url)` combinations will be fetched, parsed, embedded, and inserted into the DB.

- **Removing libraries:**  
  If you delete entries from the list, the crawler will no longer fetch them, but their rows remain in your table until you manually clean them.

---

## 2. Database Conflict Handling

The insertion SQL in `insert_docs_to_db` looks like:

```sql
INSERT INTO context7_docs
  (library_id, title, description, source_url, language, code, embedding)
VALUES %s
ON CONFLICT (library_id, source_url) DO NOTHING;
```

`ON CONFLICT ... DO NOTHING` means:

- If the pair `(library_id, source_url)` does not exist → insert as new row.
- If it does exist → skip that row, leaving the existing embedding and metadata entirely untouched.

This guarantees **idempotent runs**:

- You can re-run the crawler any number of times with the same list and never overwrite or duplicate data.
- New library IDs or newly added snippets automatically get inserted.

---

## 3. Refreshing Existing Entries

If you later need to update embeddings or metadata for libraries you’ve already crawled, you have two options:

### A. Change to DO UPDATE

Modify the SQL to refresh on conflict:

```diff
- ON CONFLICT (library_id, source_url) DO NOTHING;
+ ON CONFLICT (library_id, source_url)
+   DO UPDATE
+     SET title       = EXCLUDED.title,
+         description = EXCLUDED.description,
+         language    = EXCLUDED.language,
+         code        = EXCLUDED.code,
+         embedding   = EXCLUDED.embedding;
```

With this change:

- Every snippet is re-embedded and the row is overwritten with the fresh data.
- Existing rows are updated instead of skipped.

### B. Selective Updates

If you only want to refresh certain libraries or snippets:

- Keep `DO NOTHING`.
- Manually delete the rows you wish to re-process:

```sql
DELETE FROM context7_docs
WHERE library_id = '/angular/angular';
```

- Re-run the crawler — only those deleted rows will be re-inserted and embedded.

---

## 4. Typical Workflow

- **Initial crawl:**  
  Populate your empty `context7_docs` table.

- **Adding new libraries:**  
  Edit `CONTEXT7_LIBRARY_IDS`, re-run.  
  New rows appear, old rows stay.

- **Full refresh (optional):**  
  Switch to `DO UPDATE` for automatic overwrite on every run.

- **Targeted refresh:**  
  Manually delete specific rows; leave `DO NOTHING` in place.

---

## 5. Summary

- New list entries → inserted automatically, without disturbing existing data.
- `DO NOTHING` = protects existing rows.
- Switch to `DO UPDATE` = forces all rows to be overwritten.
- Manual deletes = give you finer control while keeping the default safe behavior.
